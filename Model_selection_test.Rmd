
---
title: "Selection Report Draft"
author: "Jennifer Levy"
date: "May 24, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

________________________________________________________________
###Load data files and install libraries for data processing. Look at the dataset. Note that quality checks were performed on the file. All model/day combinations are present in this file.
________________________________________________________________
```{r eval=TRUE, echo=FALSE,message=FALSE,warning=FALSE, cache=TRUE}
setwd("F:/BrightWater/data/model_selection_data")
a<-read.csv("mlmodels.csv", header=TRUE)
x<-read.csv("predict_us.csv", header=TRUE)
c<-read.csv("predict_county_gt0_test.csv", header=TRUE)
colnames(c)[1]<-"model_id"
data<-subset(c, c$model_id==972)
data$residuals<-data$actual_production-data$production_predicted

# install packages and libraries needed for analysis
library(plyr)
library(ModelMetrics)
library(dplyr)
library(ggplot2)
library("e1071")
```

```{r eval=TRUE, echo=FALSE,message=FALSE,warning=FALSE, cache=TRUE}
head(x)
```
________________________________________________________________
###Calculate RMSE based on weighted mean using days.
________________________________________________________________
```{r eval=TRUE, echo=TRUE,message=FALSE,warning=FALSE, cache=TRUE}

# Calculate RMSE average. This averages the RMSE for each year of the LOOCV analysis for each day and model
rmse_avg<-aggregate(x = x$rmse_weighted_by_product, by = list(x$model_id,x$doy), FUN=mean)  
colnames(rmse_avg)<-c("model_id","doy","rmse_avg")

# Calculate the the weighted rmse error for each model
wt<-c(.1,.35,.1,.35,.05,.05)

# Order days from lowest to hightest so weights are applied appropriately
rmse_avg<-rmse_avg[order(rmse_avg$model_id,rmse_avg$doy),]

# Calcualte weighted mean for each model
Results<-
        rmse_avg %>%
        group_by(model_id) %>%
        summarise(
                model_RMSE= weighted.mean(rmse_avg,wt)
        )
```

### Look at the Results file. Note that a check on the weighted mean calculation was performed and the code worked.

```{r eval=TRUE, echo=FALSE,message=FALSE,warning=FALSE, cache=TRUE}
head(Results)
```

### Merge model RMSE values to the dataset containing model settings. Rank the models according to lowest RMSE value. Look at the data file to see the structure.  

```{r eval=TRUE, echo=FALSE,message=FALSE,warning=FALSE, cache=TRUE}

# reduce dataset to remove duplicated rows
a<-a[!duplicated(a), ]

# add model RMSE values to the model ID dataset
Results<-as.data.frame(Results)
colnames(a)[8]<-"model_id"
m<-merge(a,Results,by="model_id")
# sort dataframe by lowest to hightest model RMSE
m<-m[ order(m$model_RMSE), ]
# add a colum ranking hte models
m$rank<-c(1:length(m$model_id))

# reduce the dataset to columns that vary in parameters
m2<-m[,c(1:2,4:5,8:10)]
```
```{r eval=TRUE, echo=FALSE,message=FALSE,warning=FALSE, cache=TRUE}
head(m2)
```

### Look at the range of model RMSE values.
```{r eval=TRUE, echo=FALSE,message=FALSE,warning=FALSE, cache=TRUE}

range(m2$model_RMSE)

```

### Use the top 5% of models based on RMSE to narrow down model selection. Models producing values to the left of the red line will be considered for the best model. Model settings that are not found in the top 5% will be omitted from further model testing. 

```{r eval=TRUE, echo=FALSE,message=FALSE,warning=FALSE, cache=TRUE}
plot(m$model_RMSE, type = "l",xlab="Model Index",ylab="RMSE")
abline(v=97, col="red")

```

###Create a new dataset containing only the top 5% of models.Identify how many unique datasets are in this group. 
Varibales that vary are 

* dataset
* machine learning algorithm and settings
* crop mask algorithm
* crop mask threshold settings

###Theoretically the biggest driver of model performance should be based on the input dataset rather than variation in model settings. Therefore, for each dataset, select the corn_id_model, machine learning settings, and crop_threshold combination that that produces the lowest RMSE.

```{r eval=TRUE, echo=FALSE,message=FALSE,warning=FALSE, cache=TRUE}
top5<-m2[m2$rank<97,] 

# for each dataset, select the corn_id_model,ml_model settings, and crop_threshold that produces the best estimate

t1<-top5[top5$dataset=="[evi, evi_delta, evi_std, lstd, lstn, _year, yield_average]\n",]
t2<-top5[top5$dataset=="[evi, evi_delta, evi_std, _year, yield_average]\n",]
t3<-top5[top5$dataset=="[evi, evi_delta, evi_std, lstd, _year, yield_average]\n",]
t4<-top5[top5$dataset=="[evi, evi_average, evi_average_for_doys, evi_std, _year, yield_average]\n",]
t5<-top5[top5$dataset=="[evi, evi_average, evi_average_for_doys, evi_std, lstd, _year, yield_average]\n",]
t6<-top5[top5$dataset=="[evi, evi_average, evi_std, _year, yield_average, lstd]\n",]
```

### Here are the settings and percent of occurance in the top 5% of models.
```{r eval=TRUE, echo=FALSE,message=FALSE,warning=FALSE, cache=TRUE}
ng2<-as.data.frame((table(top5$crop_threshold)))
ng2<-ng2[with(ng2, order(-Freq)), ]
ng2$percent<-round(ng2$Freq/96 *100, digits=2)
ct<-ng2
colnames(ct)<-c("Crop threshold","Frequency","Percent occurance")

ng3<-as.data.frame((table(top5$dataset)))
ng3<-ng3[with(ng3, order(-Freq)), ]
ng3$percent<-round(ng3$Freq/96 *100, digits=2)
dataset<-ng3
colnames(dataset)<-c("Dataset","Frequency","Percent occurance")

ng4<-as.data.frame((table(top5$corn_id_model)))
ng4<-ng4[with(ng4, order(-Freq)), ]
ng4$percent<-round(ng4$Freq/96 *100, digits=2)
cornID<-ng4
colnames(cornID)<-c("Corn algorithm","Frequency","Percent occurance")

ng5<-as.data.frame((table(top5$ml_models)))
ng5<-ng5[with(ng5, order(-Freq)), ]
ng5$percent<-round(ng5$Freq/96 *100, digits=2)
mlmodel<-ng5
colnames(mlmodel)<-c("ML settings","Frequency","Percent occurance")

```
```{r eval=TRUE, echo=FALSE,message=FALSE,warning=FALSE, cache=TRUE}
ct
cornID
mlmodel
dataset

```

### Create a shortlist of models to examine based on the best combination of model settings for each dataset
```{r eval=TRUE, echo=FALSE,message=FALSE,warning=FALSE, cache=TRUE}
shortlist<-rbind(t1[1,],t2[1,],t3[1,],t4[1,],t5[1,],t6[1,])
shortlist
```
### Notice that all of the model settings are consistant with the settings that appear most often in the top 5% . Since the most frequent settings in the top 5% of models produce the best RMSE for each dataset, this proivdes some confidence for the model settings under considerations based on consistent performance relative to the other options. 

### To start, the residuals from the top 4 models (containing ranks 1,3,6 and 8) will be considered. 
```{r eval=TRUE, echo=FALSE,message=FALSE,warning=FALSE, cache=TRUE}
shortlist2<-rbind(t1[1,],t2[1,],t3[1,],t4[1,])
shortlist2
```

### Results for all four models look similar so I will just show the results for the top ranking model. 

###First, Check predicted production vs observed production for each day. The results look good. 

```{r eval=TRUE, echo=FALSE,message=FALSE,warning=FALSE, cache=TRUE, fig.height=4, fig.width =12}
par(mfrow=c(1,2))

     for( n in 1:length(unique(data$doy))){
      r<-subset(data,data$doy == unique(data$doy)[n])

      reg<-lm(production_predicted ~ actual_production, data = r)
     plot(r$production_predicted,r$actual_production, xlab="Predicted Production (bushels)", ylab="Actual Production (bushels)", main = paste("DOY:", unique(data$doy)[n]))+abline(reg, col="red")  


     }
                              
```

###Check histogram distribution for each day. The residuals are centered around 0 but there are some outliers beyond the 2-4 standard deviation expected. The red lines show the maximum andminimum residual values. 

```{r eval=TRUE, echo=FALSE,message=FALSE,warning=FALSE, cache=TRUE, fig.height=4, fig.width =12}
par(mfrow=c(1,2))
     for( n in 1:length(unique(data$doy))){
             
      r<-subset(data,data$doy == unique(data$doy)[n])

     Standard_deviation<-round(c((min(r$residuals)-mean(r$residuals))/sd(r$residuals),(max(r$residuals)-mean(r$residuals))/sd(r$residuals)),2)
     
      hist(r$residuals, xlab="Production residuals (bushels)",main = paste("DOY:", unique(data$doy)[n],
 "\n Standard deviations for max residual:",Standard_deviation[2],"\n Standard deviations for min residual:",Standard_deviation[1]),include.lowest=TRUE,breaks=100, xlim=c(min(r$residuals),max(r$residuals)))
      
abline(v=min(r$residuals), col="red")
abline(v=max(r$residuals), col="red") 
     }
```
                              

### Q-Q plots for each day 
```{r eval=TRUE, echo=FALSE,message=FALSE,warning=FALSE, cache=TRUE, fig.height=4, fig.width =12}


     # calculate Quanitiles for qqline for model and day
df2 <- ddply(.data = data, .variables = .(doy), function(dat){
             q <- qqnorm(dat$residuals, plot = FALSE)
             dat$xq <- q$x
             dat
}
)

#Q-Q plot for each model
ggplot(data = df2, aes(x = xq, y = residuals)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  xlab("Theoretical") +
  ylab("Residuals(bushels)") +
  facet_grid(model_id~doy) 
 
     
```